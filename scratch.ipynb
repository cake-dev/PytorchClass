{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple pytorch neural network for adding two numbers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# define the neural network\n",
    "class Adder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Adder, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "# create the neural network\n",
    "net = Adder()\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# create the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# train the neural network\n",
    "for i in range(10000):\n",
    "    x = torch.tensor(np.random.rand(2), dtype=torch.float32)\n",
    "    y = x.sum()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = net(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('loss:', loss.item())\n",
    "\n",
    "# test the neural network\n",
    "x = torch.tensor([0.1, 0.2], dtype=torch.float32)\n",
    "y = x.sum()\n",
    "y_pred = net(x)\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('y_pred:', y_pred)\n",
    "print('error:', y_pred.item() - y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viszulize the neural network using torchviz\n",
    "from torchviz import make_dot\n",
    "x = torch.tensor([0.1, 0.2], dtype=torch.float32)\n",
    "y = x.sum()\n",
    "y_pred = net(x)\n",
    "make_dot(y_pred, params=dict(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider, Dropdown, fixed\n",
    "\n",
    "# ---------------------\n",
    "# Generate synthetic data\n",
    "# ---------------------\n",
    "def generate_data(num_samples=200, noise=0.1):\n",
    "    # Two-class data\n",
    "    x = np.random.rand(num_samples, 2) * 2 - 1\n",
    "    y = (x[:, 0] * x[:, 1] > 0).astype(np.float32)  # XOR-like pattern with noise\n",
    "    # Add small random noise\n",
    "    x += np.random.randn(*x.shape) * noise\n",
    "    return x, y\n",
    "\n",
    "# ---------------------\n",
    "# Define a small neural network\n",
    "# ---------------------\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=16, output_dim=1):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# ---------------------\n",
    "# Training function\n",
    "# ---------------------\n",
    "def train_model(optimizer_name, lr, epochs, batch_size):\n",
    "    # Data\n",
    "    x_data, y_data = generate_data(num_samples=200, noise=0.1)\n",
    "    x_tensor = torch.from_numpy(x_data).float()\n",
    "    y_tensor = torch.from_numpy(y_data).float().view(-1, 1)\n",
    "\n",
    "    model = SimpleMLP()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Select optimizer\n",
    "    if optimizer_name == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == \"RMSProp\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)  # default fallback\n",
    "\n",
    "    losses = []\n",
    "    model.train()\n",
    "    dataset_size = x_tensor.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch updates\n",
    "        perm = torch.randperm(dataset_size)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            batch_x = x_tensor[idx]\n",
    "            batch_y = y_tensor[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / (dataset_size // batch_size)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(losses, label=f'{optimizer_name} (lr={lr})')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Decision boundary visualization\n",
    "    model.eval()\n",
    "    x_min, x_max = x_data[:, 0].min() - 0.2, x_data[:, 0].max() + 0.2\n",
    "    y_min, y_max = x_data[:, 1].min() - 0.2, x_data[:, 1].max() + 0.2\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    grid = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
    "    preds = model(grid).detach().numpy().reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.contourf(xx, yy, preds, levels=[0, 0.5, 1], alpha=0.5, cmap=\"RdBu\")\n",
    "    plt.scatter(x_data[:,0], x_data[:,1], c=y_data, edgecolors='k', cmap=\"RdBu\")\n",
    "    plt.title(\"Decision Regions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Interactive widget\n",
    "# ---------------------\n",
    "def interactive_demo(optimizer_name, lr, epochs, batch_size):\n",
    "    train_model(optimizer_name, lr, epochs, batch_size)\n",
    "\n",
    "optimizer_options = [\"SGD\", \"Adam\", \"RMSProp\"]\n",
    "lr_slider = FloatSlider(value=0.01, min=0.0001, max=0.1, step=0.001, description='Learning Rate')\n",
    "epoch_slider = IntSlider(value=30, min=1, max=200, step=1, description='Epochs')\n",
    "batch_slider = IntSlider(value=32, min=1, max=128, step=1, description='Batch Size')\n",
    "\n",
    "demo = interactive(\n",
    "    interactive_demo,\n",
    "    optimizer_name=Dropdown(options=optimizer_options, value=\"SGD\", description='Optimizer'),\n",
    "    lr=lr_slider,\n",
    "    epochs=epoch_slider,\n",
    "    batch_size=batch_slider\n",
    ")\n",
    "\n",
    "demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimizer Parameters Explained\n",
    "\n",
    "## Optimizer Parameters\n",
    "\n",
    "| Parameter | Description | Default | Effect | When to Adjust |\n",
    "|-----------|-------------|---------|--------|----------------|\n",
    "| **Learning Rate (lr)** | Controls the step size for parameter updates | 0.001 | **Too high**: Oscillation or divergence<br>**Too low**: Slow convergence, may get stuck<br>**Just right**: Steady progress | Increase if loss decreases too slowly; decrease if training is unstable |\n",
    "| **Beta1 (β₁)** | Decay rate for the first moment estimate (momentum) | 0.9 | **Higher values** (closer to 1): Stronger momentum, smoother path<br>**Lower values**: More responsive to recent gradients | Increase when loss curve is noisy; decrease when stuck in plateaus |\n",
    "| **Beta2 (β₂)** | Decay rate for the second moment estimate (adaptive learning rates) | 0.999 | **Higher values**: More stable, slower adaptation<br>**Lower values**: Faster adaptation, potentially less stable | Rarely need to modify; lower if features have very different scales |\n",
    "| **Epsilon (ε)** | Small constant for numerical stability | 1e-8 | Prevents division by zero<br>Higher values reduce effective learning rate for small gradients | Rarely needs changing; increase if experiencing numerical instability |\n",
    "| **Weight Decay** | L2 regularization term to prevent overfitting | 0.0 | **Higher values**: Stronger regularization, simpler boundaries<br>**Lower values**: Less regularization, more complex models | Increase when overfitting; decrease when underfitting |\n",
    "\n",
    "## Data and Network Details\n",
    "\n",
    "| Component | Description | Details |\n",
    "|-----------|-------------|---------|\n",
    "| **Synthetic Dataset** | Binary classification with XOR-like pattern | • **Features**: 2D input space (x,y coordinates)<br>• **Samples**: 200 data points<br>• **Pattern**: Classification based on product of coordinates<br>• **Noise**: Random Gaussian noise added<br>• **Difficulty**: Non-linearly separable |\n",
    "| **Neural Network** | Simple Multi-Layer Perceptron (MLP) | • **Input layer**: 2 neurons<br>• **Hidden layer**: 16 neurons with ReLU activation<br>• **Output layer**: 1 neuron with Sigmoid activation<br>• **Parameters**: ~65 trainable parameters<br>• **Loss Function**: Binary Cross-Entropy (BCE) |\n",
    "| **Training Process** | Batch training over multiple epochs | • **Batch Size**: Controls samples per update<br>  - Larger: Smoother updates, better estimates<br>  - Smaller: Noisier updates, can escape local minima<br>• **Epochs**: Complete passes through dataset |\n",
    "\n",
    "## Visualization Panels\n",
    "\n",
    "| Panel | Description | What to Look For |\n",
    "|-------|-------------|------------------|\n",
    "| **Training Loss** | Shows loss decrease over epochs | Smooth decreasing curve indicates good learning |\n",
    "| **First Moment** | Shows momentum term evolution | Stabilizes as training progresses |\n",
    "| **Second Moment** | Shows adaptive learning rate evolution | Grows when gradients are consistent |\n",
    "| **Decision Regions** | Visualizes classification boundary | Should separate the XOR-like pattern correctly |\n",
    "\n",
    "## Tips for Experimentation\n",
    "\n",
    "| Tip | Description |\n",
    "|-----|-------------|\n",
    "| 1 | Start with default Adam parameters (lr=0.001, β₁=0.9, β₂=0.999) |\n",
    "| 2 | Modify learning rate first if training is too slow or unstable |\n",
    "| 3 | For this toy problem, 30-50 epochs are usually sufficient |\n",
    "| 4 | Watch how the decision boundary evolves with different parameters |\n",
    "| 5 | Note how moment magnitudes correlate with learning efficiency |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider, IntSlider, fixed, VBox, HBox, Output\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# ---------------------\n",
    "# Generate synthetic data\n",
    "# ---------------------\n",
    "def generate_data(num_samples=200, noise=0.1):\n",
    "    # Two-class data\n",
    "    x = np.random.rand(num_samples, 2) * 2 - 1\n",
    "    y = (x[:, 0] * x[:, 1] > 0).astype(np.float32)  # XOR-like pattern with noise\n",
    "    # Add small random noise\n",
    "    x += np.random.randn(*x.shape) * noise\n",
    "    return x, y\n",
    "\n",
    "# ---------------------\n",
    "# Define a small neural network\n",
    "# ---------------------\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=16, output_dim=1):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ---------------------\n",
    "# Training function with Adam-specific tracking\n",
    "# ---------------------\n",
    "def train_model(lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0, \n",
    "                epochs=30, batch_size=32, seed=42):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Data\n",
    "    x_data, y_data = generate_data(num_samples=200, noise=0.1)\n",
    "    x_tensor = torch.from_numpy(x_data).float()\n",
    "    y_tensor = torch.from_numpy(y_data).float().view(-1, 1)\n",
    "\n",
    "    model = SimpleMLP()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Initialize Adam optimizer with specified parameters\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=lr, \n",
    "        betas=(beta1, beta2), \n",
    "        eps=eps, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # For tracking optimizer state\n",
    "    tracked_param = next(model.parameters())\n",
    "    \n",
    "    losses = []\n",
    "    param_updates = []\n",
    "    first_moments = []\n",
    "    second_moments = []\n",
    "    update_ratios = []\n",
    "    \n",
    "    model.train()\n",
    "    dataset_size = x_tensor.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Mini-batch updates\n",
    "        perm = torch.randperm(dataset_size)\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            batch_x = x_tensor[idx]\n",
    "            batch_y = y_tensor[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Record state before optimizer step\n",
    "            if i == 0:  # Only track the first batch per epoch for clarity\n",
    "                with torch.no_grad():\n",
    "                    # Store the gradient for later use (for parameter tracking)\n",
    "                    grad_norm = torch.norm(tracked_param.grad).item()\n",
    "                \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track optimizer state after step\n",
    "            if i == 0:\n",
    "                # Access Adam state dict for the tracked parameter\n",
    "                adam_state = optimizer.state[tracked_param]\n",
    "                \n",
    "                if 'exp_avg' in adam_state:\n",
    "                    # First moment estimate (momentum)\n",
    "                    m_norm = torch.norm(adam_state['exp_avg']).item()\n",
    "                    first_moments.append(m_norm)\n",
    "                    \n",
    "                    # Second moment estimate\n",
    "                    v_norm = torch.norm(adam_state['exp_avg_sq']).item()\n",
    "                    second_moments.append(v_norm)\n",
    "                    \n",
    "                    # Calculate update magnitude ratio (m/sqrt(v))\n",
    "                    if v_norm > 0:\n",
    "                        update_ratio = m_norm / (np.sqrt(v_norm) + eps)\n",
    "                        update_ratios.append(update_ratio)\n",
    "                    else:\n",
    "                        update_ratios.append(0)\n",
    "                \n",
    "                # Parameter update magnitude\n",
    "                param_updates.append(grad_norm)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / (dataset_size // batch_size)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "    # Create multi-part figure for visualization\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    ax1.plot(losses)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(f\"Training Loss (Adam lr={lr}, β1={beta1}, β2={beta2})\")\n",
    "    \n",
    "    # Plot 2: Adam's First Moment (momentum) Magnitude\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    if first_moments:\n",
    "        ax2.plot(first_moments, label=\"First Moment (m)\")\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Magnitude\")\n",
    "        ax2.set_title(\"Adam's First Moment Magnitude\")\n",
    "        ax2.legend()\n",
    "    \n",
    "    # Plot 3: Adam's Second Moment Magnitude\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    if second_moments:\n",
    "        ax3.plot(second_moments, label=\"Second Moment (v)\")\n",
    "        ax3.set_xlabel(\"Epoch\")\n",
    "        ax3.set_ylabel(\"Magnitude\")\n",
    "        ax3.set_title(\"Adam's Second Moment Magnitude\")\n",
    "        ax3.legend()\n",
    "    \n",
    "    # Plot 4: Decision boundary visualization\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    model.eval()\n",
    "    x_min, x_max = x_data[:, 0].min() - 0.2, x_data[:, 0].max() + 0.2\n",
    "    y_min, y_max = x_data[:, 1].min() - 0.2, x_data[:, 1].max() + 0.2\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    grid = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
    "    preds = model(grid).detach().numpy().reshape(xx.shape)\n",
    "\n",
    "    ax4.contourf(xx, yy, preds, levels=[0, 0.5, 1], alpha=0.5, cmap=\"RdBu\")\n",
    "    ax4.scatter(x_data[:,0], x_data[:,1], c=y_data, edgecolors='k', cmap=\"RdBu\")\n",
    "    ax4.set_title(\"Decision Regions\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a text description of Adam's algorithm and parameters\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.axis('off')\n",
    "    adam_description = (\n",
    "        f\"Adam Optimizer Parameters Used:\\n\"\n",
    "        f\"  - Learning Rate (lr): {lr}\\n\"\n",
    "        f\"  - Beta1 (β₁): {beta1} (decay rate for first moment estimate)\\n\"\n",
    "        f\"  - Beta2 (β₂): {beta2} (decay rate for second moment estimate)\\n\"\n",
    "        f\"  - Epsilon (ε): {eps} (numerical stability term)\\n\"\n",
    "        f\"  - Weight Decay: {weight_decay}\\n\\n\"\n",
    "        f\"Adam Algorithm Summary:\\n\"\n",
    "        f\"  1. Calculates first moment (mean) of gradients with decay rate β₁\\n\"\n",
    "        f\"  2. Calculates second moment (variance) of gradients with decay rate β₂\\n\"\n",
    "        f\"  3. Applies bias correction to both moments\\n\"\n",
    "        f\"  4. Updates parameters using the bias-corrected moments\"\n",
    "    )\n",
    "    plt.text(0.05, 0.5, adam_description, fontsize=12, va='center', ha='left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------\n",
    "# Interactive widget\n",
    "# ---------------------\n",
    "def interactive_demo():\n",
    "    output = Output()\n",
    "    \n",
    "    # Define the interactive widgets\n",
    "    lr_slider = FloatSlider(value=0.01, min=0.0001, max=0.1, step=0.001, description='Learning Rate')\n",
    "    beta1_slider = FloatSlider(value=0.9, min=0.5, max=0.999, step=0.01, description='Beta1 (β₁)')\n",
    "    beta2_slider = FloatSlider(value=0.999, min=0.9, max=0.9999, step=0.0001, description='Beta2 (β₂)')\n",
    "    eps_slider = FloatSlider(value=1e-8, min=1e-10, max=1e-6, step=1e-10, \n",
    "                           description='Epsilon (ε)', readout_format='.1e')\n",
    "    weight_decay_slider = FloatSlider(value=0.0, min=0.0, max=0.1, step=0.001, description='Weight Decay')\n",
    "    epochs_slider = IntSlider(value=30, min=5, max=200, step=5, description='Epochs')\n",
    "    batch_slider = IntSlider(value=32, min=1, max=128, step=8, description='Batch Size')\n",
    "    \n",
    "    # Function to handle updating the output\n",
    "    def update_output(**kwargs):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            train_model(**kwargs)\n",
    "    \n",
    "    # Create the interactive widget\n",
    "    widget = interactive(\n",
    "        update_output,\n",
    "        lr=lr_slider,\n",
    "        beta1=beta1_slider,\n",
    "        beta2=beta2_slider,\n",
    "        eps=eps_slider,\n",
    "        weight_decay=weight_decay_slider,\n",
    "        epochs=epochs_slider,\n",
    "        batch_size=batch_slider,\n",
    "        seed=fixed(42)  # Fixed seed for reproducibility\n",
    "    )\n",
    "    \n",
    "    # Layout the widgets in a nice format\n",
    "    controls = VBox([\n",
    "        HBox([lr_slider, epochs_slider]),\n",
    "        HBox([beta1_slider, beta2_slider]),\n",
    "        HBox([eps_slider, weight_decay_slider]),\n",
    "        batch_slider\n",
    "    ])\n",
    "    \n",
    "    # Create the main UI\n",
    "    main_ui = VBox([controls, output])\n",
    "    display(main_ui)\n",
    "    \n",
    "    # Initial call to display the visualization\n",
    "    update_output(\n",
    "        lr=lr_slider.value,\n",
    "        beta1=beta1_slider.value, \n",
    "        beta2=beta2_slider.value,\n",
    "        eps=eps_slider.value,\n",
    "        weight_decay=weight_decay_slider.value,\n",
    "        epochs=epochs_slider.value,\n",
    "        batch_size=batch_slider.value,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "# Run the interactive demo\n",
    "if __name__ == \"__main__\":\n",
    "    interactive_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
